{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e581aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Load data\n",
    "data_training=pd.read_csv('census-income-training.csv',header=0)\n",
    "\n",
    "#Clean data\n",
    "#replace invalid or missing entries(delete individual row)\n",
    "data_training_clean=data_training.replace(regex=[r'\\?'],value=np.nan)\n",
    "data_training=data_training_clean.dropna(how='any')\n",
    "\n",
    "\n",
    "# Split the data into features and target label\n",
    "income_raw = data_training['income_morethan_50K']\n",
    "features_raw = data_training.drop(['income_morethan_50K','Id','GRINREG','MIGMTR1','MIGMTR3','MIGSAME','PARENT','PEFNTVTY','PEMNTVTY','PENATVTY'],axis=1)\n",
    "\n",
    "\n",
    "# Visualize skewed continuous features of original data\n",
    "#vs.distribution(data)\n",
    "\n",
    "# Log-transform the skewed features\n",
    "skewed = ['AHRSPAY','CAPGAIN', 'CAPLOSS','DIVVAL']\n",
    "features_log_transformed = pd.DataFrame(features_raw)\n",
    "features_log_transformed[skewed] = features_raw[skewed].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "#Normalizing Numerical Features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler() # default=(0, 1)\n",
    "numerical = ['AAGE', 'ADTOCC', 'AHRSPAY', 'CAPGAIN','CAPLOSS', 'DIVVAL','HHDREL','NOEMP']\n",
    "features_log_minmax_transform = pd.DataFrame(features_log_transformed)\n",
    "features_log_minmax_transform[numerical] = scaler.fit_transform(features_log_transformed[numerical])\n",
    "\n",
    "# Data Preprocessing: One-hot encode the 'features_log_minmax_transform' data using pandas.get_dummies()\n",
    "features_final = pd.get_dummies(features_log_minmax_transform)\n",
    "income_final= income_raw\n",
    "\n",
    "# Print the number of features after one-hot encoding\n",
    "#encoded = list(features_final.columns)\n",
    "#print ' total features after one-hot encoding.'\n",
    "#display (encoded)\n",
    "\n",
    "#Split the Data into Training and Test Sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(features_final,income_final,test_size = 0.2,random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4094c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:11:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
      "              gamma=0, gpu_id=-1, importance_type=None,\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
      "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "#Xgboost\n",
    "from xgboost import XGBClassifier\n",
    "#View default Values\n",
    "\n",
    "xgbc=XGBClassifier()\n",
    "xgbc.fit(x_train,y_train)\n",
    "print(xgbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "243ccfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:12:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "score of XGBoost\n",
      "0.9459298871063577\n",
      "Prediction accuracy of XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97     12434\n",
      "           1       0.73      0.46      0.57      1030\n",
      "\n",
      "    accuracy                           0.95     13464\n",
      "   macro avg       0.84      0.72      0.77     13464\n",
      "weighted avg       0.94      0.95      0.94     13464\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#default\n",
    "xgbc=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
    "              gamma=0, gpu_id=-1, importance_type=None,\n",
    "              interaction_constraints='', learning_rate=0.300000012,\n",
    "              max_delta_step=0, max_depth=6, min_child_weight=1,\n",
    "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
    "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
    "\n",
    "\n",
    "xgbc.fit(x_train,y_train)\n",
    "xgbc_y_predict=xgbc.predict(x_test)\n",
    "xgbc.score(x_test,y_test)\n",
    "print('score of XGBoost')\n",
    "print(xgbc.score(x_test,y_test))\n",
    "\n",
    "#Accuracy of prediction\n",
    "from sklearn.metrics import classification_report\n",
    "print('Prediction accuracy of XGBoost')\n",
    "print(classification_report(y_test,xgbc_y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdf9d4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.930947  with:   {'max_depth': 3, 'min_child_weight': 1}\n",
      "0.931198  with:   {'max_depth': 3, 'min_child_weight': 3}\n",
      "0.931172  with:   {'max_depth': 3, 'min_child_weight': 5}\n",
      "0.930276  with:   {'max_depth': 5, 'min_child_weight': 1}\n",
      "0.930726  with:   {'max_depth': 5, 'min_child_weight': 3}\n",
      "0.930626  with:   {'max_depth': 5, 'min_child_weight': 5}\n",
      "0.928978  with:   {'max_depth': 7, 'min_child_weight': 1}\n",
      "0.929041  with:   {'max_depth': 7, 'min_child_weight': 3}\n",
      "0.928360  with:   {'max_depth': 7, 'min_child_weight': 5}\n",
      "0.926721  with:   {'max_depth': 9, 'min_child_weight': 1}\n",
      "0.926029  with:   {'max_depth': 9, 'min_child_weight': 3}\n",
      "0.925406  with:   {'max_depth': 9, 'min_child_weight': 5}\n"
     ]
    }
   ],
   "source": [
    "#max_depth å’Œmin_child_weight\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_test1 = {\n",
    " 'max_depth':list(range(3,10,2)),\n",
    " 'min_child_weight':list(range(1,6,2))\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier\n",
    "\n",
    "                        (base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
    "              gamma=0, gpu_id=-1, importance_type=None,\n",
    "              interaction_constraints='', learning_rate=0.300000012,\n",
    "              max_delta_step=0, max_depth=6, min_child_weight=1,\n",
    "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
    "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None), \n",
    "                        \n",
    "              param_grid = param_test1, scoring='roc_auc',n_jobs=4,refit=False, cv=5)\n",
    "\n",
    "gsearch1.fit(x_train,y_train)\n",
    "means = gsearch1.cv_results_['mean_test_score']\n",
    "params = gsearch1.cv_results_['params']\n",
    "for mean,param in zip(means,params):\n",
    "    print(\"%f  with:   %r\" % (mean,param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0e1c484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.928396  with:   {'max_depth': 2, 'min_child_weight': 2}\n",
      "0.928569  with:   {'max_depth': 2, 'min_child_weight': 3}\n",
      "0.928668  with:   {'max_depth': 2, 'min_child_weight': 4}\n",
      "0.928699  with:   {'max_depth': 2, 'min_child_weight': 5}\n",
      "0.930947  with:   {'max_depth': 3, 'min_child_weight': 2}\n",
      "0.931198  with:   {'max_depth': 3, 'min_child_weight': 3}\n",
      "0.931257  with:   {'max_depth': 3, 'min_child_weight': 4}\n",
      "0.931172  with:   {'max_depth': 3, 'min_child_weight': 5}\n",
      "0.931907  with:   {'max_depth': 4, 'min_child_weight': 2}\n",
      "0.932053  with:   {'max_depth': 4, 'min_child_weight': 3}\n",
      "0.931353  with:   {'max_depth': 4, 'min_child_weight': 4}\n",
      "0.931470  with:   {'max_depth': 4, 'min_child_weight': 5}\n"
     ]
    }
   ],
   "source": [
    "#max_depth å’Œmin_child_weight again\n",
    "param_test2 = {\n",
    " 'max_depth':list(range(2,5,1)),\n",
    " 'min_child_weight':list(range(2,6,1))\n",
    "}\n",
    "gsearch2 = GridSearchCV(estimator = XGBClassifier\n",
    "\n",
    "                        (base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
    "              gamma=0, gpu_id=-1, importance_type=None,\n",
    "              interaction_constraints='', learning_rate=0.300000012,\n",
    "              max_delta_step=0, max_depth=6, min_child_weight=1,\n",
    "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
    "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None), \n",
    "                        \n",
    "              param_grid = param_test2, scoring='roc_auc',n_jobs=4,refit=False, cv=5)\n",
    "                        \n",
    "gsearch2.fit(x_train,y_train)\n",
    "means = gsearch2.cv_results_['mean_test_score']\n",
    "params = gsearch2.cv_results_['params']\n",
    "for mean,param in zip(means,params):\n",
    "    print(\"%f  with:   %r\" % (mean,param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "092d58e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.932053  with:   {'gamma': 0.0}\n",
      "0.931983  with:   {'gamma': 0.1}\n",
      "0.931987  with:   {'gamma': 0.2}\n",
      "0.931836  with:   {'gamma': 0.3}\n",
      "0.931628  with:   {'gamma': 0.4}\n",
      "0.931707  with:   {'gamma': 0.5}\n"
     ]
    }
   ],
   "source": [
    "#gamma\n",
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,6)]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = XGBClassifier\n",
    "\n",
    "                        (base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
    "              gamma=0, gpu_id=-1, importance_type=None,\n",
    "              interaction_constraints='', learning_rate=0.300000012,\n",
    "              max_delta_step=0, max_depth=4, min_child_weight=3,\n",
    "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
    "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None), \n",
    "                        \n",
    "              param_grid = param_test3, scoring='roc_auc',n_jobs=4,refit=False, cv=5)\n",
    "\n",
    "gsearch3.fit(x_train,y_train)\n",
    "means = gsearch3.cv_results_['mean_test_score']\n",
    "params = gsearch3.cv_results_['params']\n",
    "for mean,param in zip(means,params):\n",
    "    print(\"%f  with:   %r\" % (mean,param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef7851ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.930487  with:   {'colsample_bytree': 0.75, 'subsample': 0.75}\n",
      "0.930957  with:   {'colsample_bytree': 0.75, 'subsample': 0.8}\n",
      "0.930915  with:   {'colsample_bytree': 0.75, 'subsample': 0.85}\n",
      "0.931266  with:   {'colsample_bytree': 0.75, 'subsample': 0.9}\n",
      "0.931704  with:   {'colsample_bytree': 0.75, 'subsample': 0.95}\n",
      "0.930377  with:   {'colsample_bytree': 0.8, 'subsample': 0.75}\n",
      "0.931061  with:   {'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "0.931731  with:   {'colsample_bytree': 0.8, 'subsample': 0.85}\n",
      "0.931650  with:   {'colsample_bytree': 0.8, 'subsample': 0.9}\n",
      "0.931420  with:   {'colsample_bytree': 0.8, 'subsample': 0.95}\n",
      "0.931023  with:   {'colsample_bytree': 0.85, 'subsample': 0.75}\n",
      "0.931291  with:   {'colsample_bytree': 0.85, 'subsample': 0.8}\n",
      "0.931208  with:   {'colsample_bytree': 0.85, 'subsample': 0.85}\n",
      "0.931608  with:   {'colsample_bytree': 0.85, 'subsample': 0.9}\n",
      "0.931114  with:   {'colsample_bytree': 0.85, 'subsample': 0.95}\n",
      "0.930308  with:   {'colsample_bytree': 0.9, 'subsample': 0.75}\n",
      "0.931580  with:   {'colsample_bytree': 0.9, 'subsample': 0.8}\n",
      "0.931402  with:   {'colsample_bytree': 0.9, 'subsample': 0.85}\n",
      "0.931733  with:   {'colsample_bytree': 0.9, 'subsample': 0.9}\n",
      "0.931208  with:   {'colsample_bytree': 0.9, 'subsample': 0.95}\n",
      "0.930700  with:   {'colsample_bytree': 0.95, 'subsample': 0.75}\n",
      "0.930988  with:   {'colsample_bytree': 0.95, 'subsample': 0.8}\n",
      "0.931776  with:   {'colsample_bytree': 0.95, 'subsample': 0.85}\n",
      "0.931412  with:   {'colsample_bytree': 0.95, 'subsample': 0.9}\n",
      "0.931118  with:   {'colsample_bytree': 0.95, 'subsample': 0.95}\n"
     ]
    }
   ],
   "source": [
    "#subsample,colsample_bytree\n",
    "param_test4 = {\n",
    " 'subsample':[i/100.0 for i in range(75,100,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(75,100,5)]\n",
    "}\n",
    "\n",
    "gsearch4 = GridSearchCV(estimator = XGBClassifier\n",
    "\n",
    "                        (base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
    "              gamma=0, gpu_id=-1, importance_type=None,\n",
    "              interaction_constraints='', learning_rate=0.300000012,\n",
    "              max_delta_step=0, max_depth=4, min_child_weight=3,\n",
    "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
    "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None), \n",
    "                        \n",
    "              param_grid = param_test4, scoring='roc_auc',n_jobs=4,refit=False, cv=5)\n",
    "\n",
    "gsearch4.fit(x_train,y_train)\n",
    "means = gsearch4.cv_results_['mean_test_score']\n",
    "params = gsearch4.cv_results_['params']\n",
    "for mean,param in zip(means,params):\n",
    "    print(\"%f  with:   %r\" % (mean,param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "965b0e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:57:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "score of XGBoost\n",
      "0.946078431372549\n",
      "Prediction accuracy of XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97     12434\n",
      "           1       0.73      0.46      0.57      1030\n",
      "\n",
      "    accuracy                           0.95     13464\n",
      "   macro avg       0.85      0.72      0.77     13464\n",
      "weighted avg       0.94      0.95      0.94     13464\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Xgboost,After the adjustment\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "xgbc=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.8, enable_categorical=False,\n",
    "              gamma=0, gpu_id=-1, importance_type=None,\n",
    "              interaction_constraints='', learning_rate=0.300000012,\n",
    "              max_delta_step=0, max_depth=4, min_child_weight=3,\n",
    "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
    "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.85,\n",
    "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
    "\n",
    "xgbc.fit(x_train,y_train)\n",
    "xgbc_y_predict=xgbc.predict(x_test)\n",
    "xgbc.score(x_test,y_test)\n",
    "print('score of XGBoost')\n",
    "print(xgbc.score(x_test,y_test))\n",
    "\n",
    "#Accuracy of prediction\n",
    "from sklearn.metrics import classification_report\n",
    "print('Prediction accuracy of XGBoost')\n",
    "print(classification_report(y_test,xgbc_y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cacc41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
